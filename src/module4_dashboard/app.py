import streamlit as st
import pandas as pd
import plotly.express as px
import os
from datetime import datetime
from dotenv import load_dotenv

# --- CORE AI IMPORTS (LCEL Architecture) ---
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import Neo4jVector
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from neo4j import GraphDatabase

load_dotenv()

# --- 1. PREMIUM UI & BRANDING ---
st.set_page_config(page_title="Support Intel Core", layout="wide")

st.markdown("""
<style>
    .stApp { background-color: #0D1117; color: #E0E0E0; font-family: 'Inter', sans-serif; }
    [data-testid="stSidebar"] { background-color: #010409; border-right: 1px solid #30363D; }
    
    /* Modern Metric Styling */
    [data-testid="stMetricValue"] { font-size: 28px; color: #58A6FF; font-weight: 700; }
    
    /* Chat Bubbles */
    .user-msg { background: #238636; color: white; padding: 12px; border-radius: 12px; margin: 8px 0; float: right; width: 85%; clear: both; }
    .bot-msg { background: #21262D; color: #E6EDF3; padding: 12px; border-radius: 12px; margin: 8px 0; float: left; width: 85%; clear: both; border: 1px solid #30363D; }
</style>
""", unsafe_allow_html=True)

# --- 2. THE INTELLIGENCE CORE ---
@st.cache_resource
def load_engine():
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vector_store = Neo4jVector.from_existing_graph(
        embedding=embeddings,
        url=os.getenv("NEO4J_URI"),
        username=os.getenv("NEO4J_USER"),
        password=os.getenv("NEO4J_PASSWORD"),
        index_name="ticket_description_vector",
        node_label="Ticket",
        text_node_properties=["rag_content"],
        embedding_node_property="embedding"
    )
    llm = ChatGroq(groq_api_key=os.getenv("GROQ_API_KEY"), model_name="llama-3.3-70b-versatile")
    retriever = vector_store.as_retriever(search_kwargs={"k": 5}) 
    
    prompt = ChatPromptTemplate.from_template("""
    You are the Support Intelligence Core. Ground all insights in the provided context.
    Identify Ticket IDs, root causes, and technical patterns. 
    Context: {context}
    Question: {question}
    Answer:""")
    
    return (
        {"context": retriever | (lambda docs: "\n\n".join(d.page_content for d in docs)), "question": RunnablePassthrough()}
        | prompt | llm | StrOutputParser()
    )

def fetch_live_insights():
    """Analytically fetches and validates data before UI rendering."""
    driver = GraphDatabase.driver(os.getenv("NEO4J_URI"), auth=(os.getenv("NEO4J_USER"), os.getenv("NEO4J_PASSWORD")))
    with driver.session() as s:
        # KPI 1: Total Volume
        t_total = s.run("MATCH (n:Ticket) RETURN count(n) as c").single()["c"]
        
        # KPI 2: Critical Priority Check
        t_critical = s.run("MATCH (t:Ticket) WHERE t.priority = 'High' OR t.priority = 'Critical' RETURN count(t) as c").single()["c"]
        
        # Chart 1: Resolution Distribution (Solved vs Open)
        status_data = s.run("MATCH (t:Ticket) RETURN t.status as Status, count(t) as Tickets").data()
        df_status = pd.DataFrame(status_data) if status_data else pd.DataFrame()
        
        # Chart 2: Product Issue Volume (Top 10)
        prod_data = s.run("MATCH (p:Product)<-[:AFFECTS]-(t:Ticket) RETURN p.name as Product, count(t) as Count ORDER BY Count DESC LIMIT 10").data()
        df_prod = pd.DataFrame(prod_data) if prod_data else pd.DataFrame()
        
    driver.close()
    return t_total, t_critical, df_status, df_prod

# --- 3. SIDEBAR: INTELLIGENCE CENTER ---
with st.sidebar:
    st.image("https://img.icons8.com/fluency/96/data-configuration.png", width=80)
    st.title("System checks")
    st.markdown("---")
    
    st.subheader("üë®‚Äçüè´ Onboarding Assistant")
    st.caption("Common Analytical Queries:")
    
    queries = {
        "Hardware Patterns": "Analyze all high-priority hardware tickets. Which specific components are failing most frequently?",
        "Product Comparison": "Compare the technical issue volume between GoPro and Dell products. What are the top 3 root causes?",
        "Open Risk Audit": "Identify all tickets that have remained 'Open' for the longest duration. Summarize the technical blockers."
    }
    
    for label, query in queries.items():
        if st.button(f"üîç {label}", use_container_width=True):
            st.session_state.onboard_q = query

    st.markdown("---")
    if st.button("üóëÔ∏è Reset Workspace", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# --- 4. TOP: DYNAMIC KPI ROW ---
t_total, t_critical, df_status, df_prod = fetch_live_insights()
st.title("AI Intelligence Support for Enterprise applications")

k1, k2, k3, k4 = st.columns(4)
with k1: 
    st.metric("Total Records", t_total)
with k2: 
    st.metric("Critical Issues", t_critical, delta=f"{int((t_critical/t_total)*100 if t_total > 0 else 0)}% of total", delta_color="inverse")
with k3: 
    st.metric("System Engine", "GraphRAG v5")
with k4: 
    st.metric("Accuracy", "100%", delta="Grounding Verified")

st.divider()

# --- 5. MIDDLE: KNOWLEDGE SEARCH ---
if "messages" not in st.session_state: st.session_state.messages = []

chat_box = st.container(height=400, border=True)
with chat_box:
    if not st.session_state.messages:
        st.markdown("### üîç Enterprise Knowledge Search")
        st.write("Cross-referencing 100+ technical nodes. Enter a query below.")
    for msg in st.session_state.messages:
        role_class = "user-msg" if msg["role"] == "user" else "bot-msg"
        st.markdown(f'<div class="{role_class}">{msg["content"]}</div>', unsafe_allow_html=True)

# Question logic
if "onboard_q" in st.session_state:
    user_input = st.session_state.pop("onboard_q")
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.spinner("ü§ñ Scanning Graph..."):
        engine = load_engine()
        answer = engine.invoke(user_input)
        st.session_state.messages.append({"role": "assistant", "content": answer})
    st.rerun()

if user_input := st.chat_input("Input complex technical query..."):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.spinner("ü§ñ Scoping Technical Matrix..."):
        engine = load_engine()
        answer = engine.invoke(user_input)
        st.session_state.messages.append({"role": "assistant", "content": answer})
    st.rerun()

st.divider()

# --- 6. BOTTOM: ANALYTICS (DATA-DRIVEN VISIBILITY) ---
st.subheader("üìä ANALYTICS")
g1, g2 = st.columns(2)

with g1:
    if not df_status.empty:
        fig1 = px.bar(df_status, x='Status', y='Tickets', title="Resolution Trends",
                     color='Status', color_discrete_map={'Solved': '#238636', 'Open': '#DA3633', 'In Progress': '#58A6FF'})
        fig1.update_layout(paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', font_color="white")
        st.plotly_chart(fig1, use_container_width=True)
    else:
        st.info("‚ö†Ô∏è Resolution Data: Insufficient records in Neo4j to generate trends.")

with g2:
    if not df_prod.empty:
        fig2 = px.bar(df_prod, x='Product', y='Count', title="Product Issue Volume (Impact Ranking)",
                     color='Count', color_continuous_scale='Blues')
        fig2.update_layout(paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', font_color="white")
        st.plotly_chart(fig2, use_container_width=True)
    # else:
    #     st.info("‚ö†Ô∏è Product Volume: No relationship data (Ticket-Product) detected.")